{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Import all the libraries needed\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import csv \n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.preprocessing import PolynomialFeatures \n",
    "from sklearn import svm \n",
    "from sklearn.neighbors import NearestNeighbors \n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.naive_bayes import GaussianNB  \n",
    "from scipy import stats \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.model_selection import KFold \n",
    "from sklearn.grid_search import GridSearchCV  \n",
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "from sklearn import metrics  \n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import data\n",
    "data = pd.read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to know the number of cases and the number of variables\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "            ...             radius_worst  texture_worst  perimeter_worst  \\\n",
       "0           ...                    25.38          17.33           184.60   \n",
       "1           ...                    24.99          23.41           158.80   \n",
       "2           ...                    23.57          25.53           152.50   \n",
       "3           ...                    14.91          26.50            98.87   \n",
       "4           ...                    22.54          16.67           152.20   \n",
       "\n",
       "   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "569\n",
      "31\n",
      "212\n",
      "357\n"
     ]
    }
   ],
   "source": [
    "#Description of the dataset\n",
    "\n",
    "#how many cases are included in the dataset\n",
    "length = len(data)\n",
    "#how many features are in the dataset\n",
    "features = data.shape[1]-1\n",
    "\n",
    "# Number of malignant cases\n",
    "malignant = len(data[data['diagnosis']=='M'])\n",
    "\n",
    "#Number of benign cases\n",
    "benign = len(data[data['diagnosis']=='B'])\n",
    "\n",
    "#Rate of malignant tumors over all cases\n",
    "rate = (float(malignant)/(length))*100\n",
    "\n",
    "print (length)\n",
    "print (features)\n",
    "print (malignant)\n",
    "print (benign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['M', 'B'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.diagnosis.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean', 'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se', 'fractal_dimension_se', 'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst', 'smoothness_worst', 'compactness_worst', 'concavity_worst', 'concave points_worst', 'symmetry_worst']\n"
     ]
    }
   ],
   "source": [
    "# Extract feature columns where everything but the diagnosis is included.\n",
    "# I am separating all the features that are helpful in determining the diagnosis\n",
    "features = list(data.columns[1:30])\n",
    "print (features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['diagnosis'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#Our target is predicting the diagnosis in benign or malignant, so we need\n",
    "#to extract this one as the dependent variable - the variable we will predict\n",
    "target = data.columns[0:1]\n",
    "print (target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we need to separate the data into feature data and target data\n",
    "X = data[features] #our features that we will use to predict Y\n",
    "Y = data[target] #our dependent variable, the one we are trying to predict from X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 29)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X should have 29 variables and 569 cases\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y should have 1 variable - just the diagnosis and 569 cases\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
      "0         17.990         10.38          122.80     1001.0          0.11840   \n",
      "1         20.570         17.77          132.90     1326.0          0.08474   \n",
      "2         19.690         21.25          130.00     1203.0          0.10960   \n",
      "3         11.420         20.38           77.58      386.1          0.14250   \n",
      "4         20.290         14.34          135.10     1297.0          0.10030   \n",
      "5         12.450         15.70           82.57      477.1          0.12780   \n",
      "6         18.250         19.98          119.60     1040.0          0.09463   \n",
      "7         13.710         20.83           90.20      577.9          0.11890   \n",
      "8         13.000         21.82           87.50      519.8          0.12730   \n",
      "9         12.460         24.04           83.97      475.9          0.11860   \n",
      "10        16.020         23.24          102.70      797.8          0.08206   \n",
      "11        15.780         17.89          103.60      781.0          0.09710   \n",
      "12        19.170         24.80          132.40     1123.0          0.09740   \n",
      "13        15.850         23.95          103.70      782.7          0.08401   \n",
      "14        13.730         22.61           93.60      578.3          0.11310   \n",
      "15        14.540         27.54           96.73      658.8          0.11390   \n",
      "16        14.680         20.13           94.74      684.5          0.09867   \n",
      "17        16.130         20.68          108.10      798.8          0.11700   \n",
      "18        19.810         22.15          130.00     1260.0          0.09831   \n",
      "19        13.540         14.36           87.46      566.3          0.09779   \n",
      "20        13.080         15.71           85.63      520.0          0.10750   \n",
      "21         9.504         12.44           60.34      273.9          0.10240   \n",
      "22        15.340         14.26          102.50      704.4          0.10730   \n",
      "23        21.160         23.04          137.20     1404.0          0.09428   \n",
      "24        16.650         21.38          110.00      904.6          0.11210   \n",
      "25        17.140         16.40          116.00      912.7          0.11860   \n",
      "26        14.580         21.53           97.41      644.8          0.10540   \n",
      "27        18.610         20.25          122.10     1094.0          0.09440   \n",
      "28        15.300         25.27          102.40      732.4          0.10820   \n",
      "29        17.570         15.05          115.00      955.1          0.09847   \n",
      "..           ...           ...             ...        ...              ...   \n",
      "539        7.691         25.44           48.34      170.4          0.08668   \n",
      "540       11.540         14.44           74.65      402.9          0.09984   \n",
      "541       14.470         24.99           95.81      656.4          0.08837   \n",
      "542       14.740         25.42           94.70      668.6          0.08275   \n",
      "543       13.210         28.06           84.88      538.4          0.08671   \n",
      "544       13.870         20.70           89.77      584.8          0.09578   \n",
      "545       13.620         23.23           87.19      573.2          0.09246   \n",
      "546       10.320         16.35           65.31      324.9          0.09434   \n",
      "547       10.260         16.58           65.85      320.8          0.08877   \n",
      "548        9.683         19.34           61.05      285.7          0.08491   \n",
      "549       10.820         24.21           68.89      361.6          0.08192   \n",
      "550       10.860         21.48           68.51      360.5          0.07431   \n",
      "551       11.130         22.44           71.49      378.4          0.09566   \n",
      "552       12.770         29.43           81.35      507.9          0.08276   \n",
      "553        9.333         21.94           59.01      264.0          0.09240   \n",
      "554       12.880         28.92           82.50      514.3          0.08123   \n",
      "555       10.290         27.61           65.67      321.4          0.09030   \n",
      "556       10.160         19.59           64.73      311.7          0.10030   \n",
      "557        9.423         27.88           59.26      271.3          0.08123   \n",
      "558       14.590         22.68           96.39      657.1          0.08473   \n",
      "559       11.510         23.93           74.52      403.5          0.09261   \n",
      "560       14.050         27.15           91.38      600.4          0.09929   \n",
      "561       11.200         29.37           70.67      386.0          0.07449   \n",
      "562       15.220         30.62          103.40      716.9          0.10480   \n",
      "563       20.920         25.09          143.00     1347.0          0.10990   \n",
      "564       21.560         22.39          142.00     1479.0          0.11100   \n",
      "565       20.130         28.25          131.20     1261.0          0.09780   \n",
      "566       16.600         28.08          108.30      858.1          0.08455   \n",
      "567       20.600         29.33          140.10     1265.0          0.11780   \n",
      "568        7.760         24.54           47.92      181.0          0.05263   \n",
      "\n",
      "     compactness_mean  concavity_mean  concave points_mean  symmetry_mean  \\\n",
      "0             0.27760        0.300100             0.147100         0.2419   \n",
      "1             0.07864        0.086900             0.070170         0.1812   \n",
      "2             0.15990        0.197400             0.127900         0.2069   \n",
      "3             0.28390        0.241400             0.105200         0.2597   \n",
      "4             0.13280        0.198000             0.104300         0.1809   \n",
      "5             0.17000        0.157800             0.080890         0.2087   \n",
      "6             0.10900        0.112700             0.074000         0.1794   \n",
      "7             0.16450        0.093660             0.059850         0.2196   \n",
      "8             0.19320        0.185900             0.093530         0.2350   \n",
      "9             0.23960        0.227300             0.085430         0.2030   \n",
      "10            0.06669        0.032990             0.033230         0.1528   \n",
      "11            0.12920        0.099540             0.066060         0.1842   \n",
      "12            0.24580        0.206500             0.111800         0.2397   \n",
      "13            0.10020        0.099380             0.053640         0.1847   \n",
      "14            0.22930        0.212800             0.080250         0.2069   \n",
      "15            0.15950        0.163900             0.073640         0.2303   \n",
      "16            0.07200        0.073950             0.052590         0.1586   \n",
      "17            0.20220        0.172200             0.102800         0.2164   \n",
      "18            0.10270        0.147900             0.094980         0.1582   \n",
      "19            0.08129        0.066640             0.047810         0.1885   \n",
      "20            0.12700        0.045680             0.031100         0.1967   \n",
      "21            0.06492        0.029560             0.020760         0.1815   \n",
      "22            0.21350        0.207700             0.097560         0.2521   \n",
      "23            0.10220        0.109700             0.086320         0.1769   \n",
      "24            0.14570        0.152500             0.091700         0.1995   \n",
      "25            0.22760        0.222900             0.140100         0.3040   \n",
      "26            0.18680        0.142500             0.087830         0.2252   \n",
      "27            0.10660        0.149000             0.077310         0.1697   \n",
      "28            0.16970        0.168300             0.087510         0.1926   \n",
      "29            0.11570        0.098750             0.079530         0.1739   \n",
      "..                ...             ...                  ...            ...   \n",
      "539           0.11990        0.092520             0.013640         0.2037   \n",
      "540           0.11200        0.067370             0.025940         0.1818   \n",
      "541           0.12300        0.100900             0.038900         0.1872   \n",
      "542           0.07214        0.041050             0.030270         0.1840   \n",
      "543           0.06877        0.029870             0.032750         0.1628   \n",
      "544           0.10180        0.036880             0.023690         0.1620   \n",
      "545           0.06747        0.029740             0.024430         0.1664   \n",
      "546           0.04994        0.010120             0.005495         0.1885   \n",
      "547           0.08066        0.043580             0.024380         0.1669   \n",
      "548           0.05030        0.023370             0.009615         0.1580   \n",
      "549           0.06602        0.015480             0.008160         0.1976   \n",
      "550           0.04227        0.000000             0.000000         0.1661   \n",
      "551           0.08194        0.048240             0.022570         0.2030   \n",
      "552           0.04234        0.019970             0.014990         0.1539   \n",
      "553           0.05605        0.039960             0.012820         0.1692   \n",
      "554           0.05824        0.061950             0.023430         0.1566   \n",
      "555           0.07658        0.059990             0.027380         0.1593   \n",
      "556           0.07504        0.005025             0.011160         0.1791   \n",
      "557           0.04971        0.000000             0.000000         0.1742   \n",
      "558           0.13300        0.102900             0.037360         0.1454   \n",
      "559           0.10210        0.111200             0.041050         0.1388   \n",
      "560           0.11260        0.044620             0.043040         0.1537   \n",
      "561           0.03558        0.000000             0.000000         0.1060   \n",
      "562           0.20870        0.255000             0.094290         0.2128   \n",
      "563           0.22360        0.317400             0.147400         0.2149   \n",
      "564           0.11590        0.243900             0.138900         0.1726   \n",
      "565           0.10340        0.144000             0.097910         0.1752   \n",
      "566           0.10230        0.092510             0.053020         0.1590   \n",
      "567           0.27700        0.351400             0.152000         0.2397   \n",
      "568           0.04362        0.000000             0.000000         0.1587   \n",
      "\n",
      "     fractal_dimension_mean       ...        fractal_dimension_se  \\\n",
      "0                   0.07871       ...                    0.006193   \n",
      "1                   0.05667       ...                    0.003532   \n",
      "2                   0.05999       ...                    0.004571   \n",
      "3                   0.09744       ...                    0.009208   \n",
      "4                   0.05883       ...                    0.005115   \n",
      "5                   0.07613       ...                    0.005082   \n",
      "6                   0.05742       ...                    0.002179   \n",
      "7                   0.07451       ...                    0.005412   \n",
      "8                   0.07389       ...                    0.003749   \n",
      "9                   0.08243       ...                    0.010080   \n",
      "10                  0.05697       ...                    0.003042   \n",
      "11                  0.06082       ...                    0.004144   \n",
      "12                  0.07800       ...                    0.012840   \n",
      "13                  0.05338       ...                    0.003002   \n",
      "14                  0.07682       ...                    0.008093   \n",
      "15                  0.07077       ...                    0.005466   \n",
      "16                  0.05922       ...                    0.002085   \n",
      "17                  0.07356       ...                    0.004142   \n",
      "18                  0.05395       ...                    0.001997   \n",
      "19                  0.05766       ...                    0.002300   \n",
      "20                  0.06811       ...                    0.002425   \n",
      "21                  0.06905       ...                    0.002968   \n",
      "22                  0.07032       ...                    0.004394   \n",
      "23                  0.05278       ...                    0.001987   \n",
      "24                  0.06330       ...                    0.002801   \n",
      "25                  0.07413       ...                    0.007444   \n",
      "26                  0.06924       ...                    0.003711   \n",
      "27                  0.05699       ...                    0.004217   \n",
      "28                  0.06540       ...                    0.002967   \n",
      "29                  0.06149       ...                    0.003742   \n",
      "..                      ...       ...                         ...   \n",
      "539                 0.07751       ...                    0.007551   \n",
      "540                 0.06782       ...                    0.005512   \n",
      "541                 0.06341       ...                    0.006111   \n",
      "542                 0.05680       ...                    0.002626   \n",
      "543                 0.05781       ...                    0.001343   \n",
      "544                 0.06688       ...                    0.003599   \n",
      "545                 0.05801       ...                    0.002583   \n",
      "546                 0.06201       ...                    0.002606   \n",
      "547                 0.06714       ...                    0.005890   \n",
      "548                 0.06235       ...                    0.004154   \n",
      "549                 0.06328       ...                    0.002977   \n",
      "550                 0.05948       ...                    0.002228   \n",
      "551                 0.06552       ...                    0.004723   \n",
      "552                 0.05637       ...                    0.001726   \n",
      "553                 0.06576       ...                    0.004623   \n",
      "554                 0.05708       ...                    0.002801   \n",
      "555                 0.06127       ...                    0.004938   \n",
      "556                 0.06331       ...                    0.002278   \n",
      "557                 0.06059       ...                    0.003324   \n",
      "558                 0.06147       ...                    0.004406   \n",
      "559                 0.06570       ...                    0.004738   \n",
      "560                 0.06171       ...                    0.005304   \n",
      "561                 0.05502       ...                    0.001773   \n",
      "562                 0.07152       ...                    0.006142   \n",
      "563                 0.06879       ...                    0.006213   \n",
      "564                 0.05623       ...                    0.004239   \n",
      "565                 0.05533       ...                    0.002498   \n",
      "566                 0.05648       ...                    0.003892   \n",
      "567                 0.07016       ...                    0.006185   \n",
      "568                 0.05884       ...                    0.002783   \n",
      "\n",
      "     radius_worst  texture_worst  perimeter_worst  area_worst  \\\n",
      "0          25.380          17.33           184.60      2019.0   \n",
      "1          24.990          23.41           158.80      1956.0   \n",
      "2          23.570          25.53           152.50      1709.0   \n",
      "3          14.910          26.50            98.87       567.7   \n",
      "4          22.540          16.67           152.20      1575.0   \n",
      "5          15.470          23.75           103.40       741.6   \n",
      "6          22.880          27.66           153.20      1606.0   \n",
      "7          17.060          28.14           110.60       897.0   \n",
      "8          15.490          30.73           106.20       739.3   \n",
      "9          15.090          40.68            97.65       711.4   \n",
      "10         19.190          33.88           123.80      1150.0   \n",
      "11         20.420          27.28           136.50      1299.0   \n",
      "12         20.960          29.94           151.70      1332.0   \n",
      "13         16.840          27.66           112.00       876.5   \n",
      "14         15.030          32.01           108.80       697.7   \n",
      "15         17.460          37.13           124.10       943.2   \n",
      "16         19.070          30.88           123.40      1138.0   \n",
      "17         20.960          31.48           136.80      1315.0   \n",
      "18         27.320          30.88           186.80      2398.0   \n",
      "19         15.110          19.26            99.70       711.2   \n",
      "20         14.500          20.49            96.09       630.5   \n",
      "21         10.230          15.66            65.13       314.9   \n",
      "22         18.070          19.08           125.10       980.9   \n",
      "23         29.170          35.59           188.00      2615.0   \n",
      "24         26.460          31.56           177.00      2215.0   \n",
      "25         22.250          21.40           152.40      1461.0   \n",
      "26         17.620          33.21           122.40       896.9   \n",
      "27         21.310          27.26           139.90      1403.0   \n",
      "28         20.270          36.71           149.30      1269.0   \n",
      "29         20.010          19.52           134.90      1227.0   \n",
      "..            ...            ...              ...         ...   \n",
      "539         8.678          31.89            54.49       223.6   \n",
      "540        12.260          19.68            78.78       457.8   \n",
      "541        16.220          31.73           113.50       808.9   \n",
      "542        16.510          32.29           107.40       826.4   \n",
      "543        14.370          37.17            92.48       629.6   \n",
      "544        15.050          24.75            99.17       688.6   \n",
      "545        15.350          29.09            97.58       729.8   \n",
      "546        11.250          21.77            71.12       384.9   \n",
      "547        10.830          22.04            71.08       357.4   \n",
      "548        10.930          25.59            69.10       364.2   \n",
      "549        13.030          31.45            83.90       505.6   \n",
      "550        11.660          24.77            74.08       412.3   \n",
      "551        12.020          28.26            77.80       436.6   \n",
      "552        13.870          36.00            88.10       594.7   \n",
      "553         9.845          25.05            62.86       295.8   \n",
      "554        13.890          35.74            88.84       595.7   \n",
      "555        10.840          34.91            69.57       357.6   \n",
      "556        10.650          22.88            67.88       347.3   \n",
      "557        10.490          34.24            66.50       330.6   \n",
      "558        15.480          27.27           105.90       733.5   \n",
      "559        12.480          37.16            82.28       474.2   \n",
      "560        15.300          33.17           100.20       706.7   \n",
      "561        11.920          38.30            75.19       439.6   \n",
      "562        17.520          42.79           128.70       915.0   \n",
      "563        24.290          29.41           179.10      1819.0   \n",
      "564        25.450          26.40           166.10      2027.0   \n",
      "565        23.690          38.25           155.00      1731.0   \n",
      "566        18.980          34.12           126.70      1124.0   \n",
      "567        25.740          39.42           184.60      1821.0   \n",
      "568         9.456          30.37            59.16       268.6   \n",
      "\n",
      "     smoothness_worst  compactness_worst  concavity_worst  \\\n",
      "0             0.16220            0.66560          0.71190   \n",
      "1             0.12380            0.18660          0.24160   \n",
      "2             0.14440            0.42450          0.45040   \n",
      "3             0.20980            0.86630          0.68690   \n",
      "4             0.13740            0.20500          0.40000   \n",
      "5             0.17910            0.52490          0.53550   \n",
      "6             0.14420            0.25760          0.37840   \n",
      "7             0.16540            0.36820          0.26780   \n",
      "8             0.17030            0.54010          0.53900   \n",
      "9             0.18530            1.05800          1.10500   \n",
      "10            0.11810            0.15510          0.14590   \n",
      "11            0.13960            0.56090          0.39650   \n",
      "12            0.10370            0.39030          0.36390   \n",
      "13            0.11310            0.19240          0.23220   \n",
      "14            0.16510            0.77250          0.69430   \n",
      "15            0.16780            0.65770          0.70260   \n",
      "16            0.14640            0.18710          0.29140   \n",
      "17            0.17890            0.42330          0.47840   \n",
      "18            0.15120            0.31500          0.53720   \n",
      "19            0.14400            0.17730          0.23900   \n",
      "20            0.13120            0.27760          0.18900   \n",
      "21            0.13240            0.11480          0.08867   \n",
      "22            0.13900            0.59540          0.63050   \n",
      "23            0.14010            0.26000          0.31550   \n",
      "24            0.18050            0.35780          0.46950   \n",
      "25            0.15450            0.39490          0.38530   \n",
      "26            0.15250            0.66430          0.55390   \n",
      "27            0.13380            0.21170          0.34460   \n",
      "28            0.16410            0.61100          0.63350   \n",
      "29            0.12550            0.28120          0.24890   \n",
      "..                ...                ...              ...   \n",
      "539           0.15960            0.30640          0.33930   \n",
      "540           0.13450            0.21180          0.17970   \n",
      "541           0.13400            0.42020          0.40400   \n",
      "542           0.10600            0.13760          0.16110   \n",
      "543           0.10720            0.13810          0.10620   \n",
      "544           0.12640            0.20370          0.13770   \n",
      "545           0.12160            0.15170          0.10490   \n",
      "546           0.12850            0.08842          0.04384   \n",
      "547           0.14610            0.22460          0.17830   \n",
      "548           0.11990            0.09546          0.09350   \n",
      "549           0.12040            0.16330          0.06194   \n",
      "550           0.10010            0.07348          0.00000   \n",
      "551           0.10870            0.17820          0.15640   \n",
      "552           0.12340            0.10640          0.08653   \n",
      "553           0.11030            0.08298          0.07993   \n",
      "554           0.12270            0.16200          0.24390   \n",
      "555           0.13840            0.17100          0.20000   \n",
      "556           0.12650            0.12000          0.01005   \n",
      "557           0.10730            0.07158          0.00000   \n",
      "558           0.10260            0.31710          0.36620   \n",
      "559           0.12980            0.25170          0.36300   \n",
      "560           0.12410            0.22640          0.13260   \n",
      "561           0.09267            0.05494          0.00000   \n",
      "562           0.14170            0.79170          1.17000   \n",
      "563           0.14070            0.41860          0.65990   \n",
      "564           0.14100            0.21130          0.41070   \n",
      "565           0.11660            0.19220          0.32150   \n",
      "566           0.11390            0.30940          0.34030   \n",
      "567           0.16500            0.86810          0.93870   \n",
      "568           0.08996            0.06444          0.00000   \n",
      "\n",
      "     concave points_worst  symmetry_worst  \n",
      "0                 0.26540          0.4601  \n",
      "1                 0.18600          0.2750  \n",
      "2                 0.24300          0.3613  \n",
      "3                 0.25750          0.6638  \n",
      "4                 0.16250          0.2364  \n",
      "5                 0.17410          0.3985  \n",
      "6                 0.19320          0.3063  \n",
      "7                 0.15560          0.3196  \n",
      "8                 0.20600          0.4378  \n",
      "9                 0.22100          0.4366  \n",
      "10                0.09975          0.2948  \n",
      "11                0.18100          0.3792  \n",
      "12                0.17670          0.3176  \n",
      "13                0.11190          0.2809  \n",
      "14                0.22080          0.3596  \n",
      "15                0.17120          0.4218  \n",
      "16                0.16090          0.3029  \n",
      "17                0.20730          0.3706  \n",
      "18                0.23880          0.2768  \n",
      "19                0.12880          0.2977  \n",
      "20                0.07283          0.3184  \n",
      "21                0.06227          0.2450  \n",
      "22                0.23930          0.4667  \n",
      "23                0.20090          0.2822  \n",
      "24                0.20950          0.3613  \n",
      "25                0.25500          0.4066  \n",
      "26                0.27010          0.4264  \n",
      "27                0.14900          0.2341  \n",
      "28                0.20240          0.4027  \n",
      "29                0.14560          0.2756  \n",
      "..                    ...             ...  \n",
      "539               0.05000          0.2790  \n",
      "540               0.06918          0.2329  \n",
      "541               0.12050          0.3187  \n",
      "542               0.10950          0.2722  \n",
      "543               0.07958          0.2473  \n",
      "544               0.06845          0.2249  \n",
      "545               0.07174          0.2642  \n",
      "546               0.02381          0.2681  \n",
      "547               0.08333          0.2691  \n",
      "548               0.03846          0.2552  \n",
      "549               0.03264          0.3059  \n",
      "550               0.00000          0.2458  \n",
      "551               0.06413          0.3169  \n",
      "552               0.06498          0.2407  \n",
      "553               0.02564          0.2435  \n",
      "554               0.06493          0.2372  \n",
      "555               0.09127          0.2226  \n",
      "556               0.02232          0.2262  \n",
      "557               0.00000          0.2475  \n",
      "558               0.11050          0.2258  \n",
      "559               0.09653          0.2112  \n",
      "560               0.10480          0.2250  \n",
      "561               0.00000          0.1566  \n",
      "562               0.23560          0.4089  \n",
      "563               0.25420          0.2929  \n",
      "564               0.22160          0.2060  \n",
      "565               0.16280          0.2572  \n",
      "566               0.14180          0.2218  \n",
      "567               0.26500          0.4087  \n",
      "568               0.00000          0.2871  \n",
      "\n",
      "[569 rows x 29 columns]\n",
      "     diagnosis\n",
      "0            1\n",
      "1            1\n",
      "2            1\n",
      "3            1\n",
      "4            1\n",
      "5            1\n",
      "6            1\n",
      "7            1\n",
      "8            1\n",
      "9            1\n",
      "10           1\n",
      "11           1\n",
      "12           1\n",
      "13           1\n",
      "14           1\n",
      "15           1\n",
      "16           1\n",
      "17           1\n",
      "18           1\n",
      "19           0\n",
      "20           0\n",
      "21           0\n",
      "22           1\n",
      "23           1\n",
      "24           1\n",
      "25           1\n",
      "26           1\n",
      "27           1\n",
      "28           1\n",
      "29           1\n",
      "..         ...\n",
      "539          0\n",
      "540          0\n",
      "541          0\n",
      "542          0\n",
      "543          0\n",
      "544          0\n",
      "545          0\n",
      "546          0\n",
      "547          0\n",
      "548          0\n",
      "549          0\n",
      "550          0\n",
      "551          0\n",
      "552          0\n",
      "553          0\n",
      "554          0\n",
      "555          0\n",
      "556          0\n",
      "557          0\n",
      "558          0\n",
      "559          0\n",
      "560          0\n",
      "561          0\n",
      "562          1\n",
      "563          1\n",
      "564          1\n",
      "565          1\n",
      "566          1\n",
      "567          1\n",
      "568          0\n",
      "\n",
      "[569 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_features(X):\n",
    "    \n",
    "    # Initialize new output DataFrame\n",
    "    output = pd.DataFrame(index = X.index)\n",
    "\n",
    "    # Investigate each feature column for the data\n",
    "    for col, col_data in X.iteritems():\n",
    "        \n",
    "        # If data type is non-numeric, replace all M/B malignant/benign values with 1/0\n",
    "        if col_data.dtype == object:\n",
    "            col_data = col_data.replace(['M', 'B'], [1, 0])\n",
    " \n",
    "        # Collect the revised columns\n",
    "        output = output.join(col_data)\n",
    "    \n",
    "    return output\n",
    "\n",
    "X = preprocess_features(X)\n",
    "Y = preprocess_features(Y)\n",
    "print (X)\n",
    "print (Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6257309941520468"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "# Divide records in training and testing sets.\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=2, stratify=Y)\n",
    "\n",
    "# Create an SVM classifier and train it on 70% of the data set.\n",
    "clf = SVC(probability=True)\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    " #Analyze accuracy of predictions on 30% of the holdout test sample.\n",
    "classifier_score = clf.score(X_test, Y_test)\n",
    "classifier_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:458: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:458: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:458: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:458: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:458: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:458: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:458: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:458: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:458: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:458: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.957244 (0.035593)\n",
      "KNN: 0.929679 (0.031181)\n",
      "CART: 0.934295 (0.046161)\n",
      "SVM: 0.628333 (0.052317)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "# Evaluate Algorithms\n",
    "models = []\n",
    "models.append(( 'LR' , LogisticRegression()))\n",
    "models.append(( 'KNN' , KNeighborsClassifier()))\n",
    "models.append(( 'CART' , DecisionTreeClassifier()))\n",
    "models.append(( 'SVM' , SVC()))\n",
    "\n",
    "\n",
    "# Test options and evaluation metric\n",
    "num_folds = 10\n",
    "num_instances = len(X_train)\n",
    "scoring =  'accuracy'\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "   \n",
    " kfold = KFold( n_splits= num_folds, shuffle= False )\n",
    " cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    " results.append(cv_results)\n",
    " names.append(name)\n",
    " msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    " print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compare Algorithms\n",
    "fig = plt.figure()\n",
    "fig.suptitle( 'Algorithm Comparison' )\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation\n",
    "The results show a similar tight distribution for all classifiers except SVM which is encouraging, suggesting low variance. The poor results for SVM are surprising.\n",
    "It is possible the varied distribution of the attributes may have an effect on the accuracy of algorithms such as SVM. In the next section we will repeat this spot-check with a standardized copy of the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py:250: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py:250: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py:250: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py:250: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py:250: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py:250: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py:250: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py:250: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py:250: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py:250: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScaledLR: 0.979744 (0.024943)\n",
      "ScaledKNN: 0.969679 (0.027347)\n",
      "ScaledCART: 0.941987 (0.028300)\n",
      "ScaledSVM: 0.979808 (0.018934)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# Standardize the dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "pipelines = []\n",
    "pipelines.append(( 'ScaledLR' , Pipeline([( 'Scaler' , StandardScaler()),( 'LR' ,\n",
    "    LogisticRegression())])))\n",
    "pipelines.append(( 'ScaledKNN' , Pipeline([( 'Scaler' , StandardScaler()),( 'KNN' ,\n",
    "    KNeighborsClassifier())])))\n",
    "pipelines.append(( 'ScaledCART' , Pipeline([( 'Scaler' , StandardScaler()),( 'CART' ,\n",
    "    DecisionTreeClassifier())])))\n",
    "pipelines.append(( 'ScaledSVM' , Pipeline([( 'Scaler' , StandardScaler()),( 'SVM' , SVC())])))\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "seed = 3\n",
    "for name, model in pipelines:\n",
    "  kfold = KFold( n_splits= num_folds, shuffle= False , random_state=seed)\n",
    "  cv_results = cross_val_score(model, X_train, Y_train, cv=kfold,\n",
    "      scoring=scoring)\n",
    "  results.append(cv_results)\n",
    "  names.append(name)\n",
    "  msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "  print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compare Algorithms\n",
    "fig = plt.figure()\n",
    "fig.suptitle( 'Scaled Algorithm Comparison' )\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation\n",
    "The results show that standardization of the data has lifted the skill of SVM to be the most accurate algorithm tested so far."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
